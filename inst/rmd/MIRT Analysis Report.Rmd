---
title: "MIRT Analysis Report"
author: "TestAnaAPP"
date: "`r format(Sys.time(), '%d/%m/%Y')`"
output: 
  officedown::rdocx_document:
    plots:
      style: Normal
      align: center
      fig.lp: 'fig:'
      topcaption: false
      caption:
        style: Image Caption
        pre: 'Figure '
        sep: ' '
        fp_text: !expr officer::fp_text_lite(bold = TRUE, italic = FALSE)
    tables:
      style: Table
      layout: autofit
      width: 1.0
      topcaption: true
      tab.lp: 'tab:'
      caption:
        style: Table Caption
        pre: 'Table '
        sep: ' '
        fp_text: !expr officer::fp_text_lite(bold = TRUE, italic = FALSE)
      conditional:
        first_row: true
        first_column: false
        last_row: false
        last_column: false
        no_hband: false
        no_vband: true
    mapstyles:
      Normal: ['First Paragraph']
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning = FALSE)
library(officedown)
library(officer)
fmt_tra_other <- function(ft, reset_width = TRUE){#Convert matrix to table.
  c <- ncol(ft)
  ft <- as.data.frame(ft)%>%flextable()
  ft <- flextable::bold(ft, part = "header")
  ft <- flextable::fontsize(ft, size = 10, part = "all")
  ft <- flextable::font(ft, fontname = "Times New Roman")
  ft <- flextable::align_nottext_col(ft, align = "center")
  #合并单元，调整格式
  ft <- flextable::merge_h(ft, part = "header")%>%
    flextable::merge_v(part = "header")%>%
    flextable::theme_booktabs()%>%
    flextable::bold( part = "header")%>% # Bold
    flextable::bold(j=1, part = "body")%>%
    #居中
    flextable::align_text_col(align = "center")%>%
    flextable::valign(valign = "center")%>%
    flextable::padding(padding = 2)
  
  if(reset_width == TRUE){
    ft <- flextable::autofit(ft)%>%
    flextable::width(j = 1:c,width = (16/2.54)/c)
  }
  return(ft)
}
bold_print <- function(text){
  text_format <- fp_text(bold = TRUE, font.family = "Time News Roman",font.size = 12)
  fp <- fp_par(text.align = "center")
  return(fpar(ftext(text, prop = text_format), fp_p = fp))
}

plot_width <- 6.299

```


`r bold_print("Table of Contents")`
<!---BLOCK_TOC--->

`r bold_print("List of Figures")`

<!---BLOCK_TOC{seq_id: 'fig'}--->

`r bold_print("List of Tables")`

<!---BLOCK_TOC{seq_id: 'tab'}--->

\newpage

Multidimensional IRT model assumes that more than one single latent trait underlies the total collection of item response. Generally, these model are classified to between-item and within-item models. 

For the between-item multidimensional model, all items were assumed to assess only one single latent trait of several traits, or dimensions, being measured by the test. For the within-item multidimensional model, in contrast, at least one item measured more than one traits or dimensions.

Considered the between-item test is universal and can save time in estimation, we used the between-item multidimensional models to analysis data in TestAnaAPP. Thus, the users of TestAnaAPP should upload the between-item test data and the information of dimension for analysis.

# Introduction to MIRT Models

TestAnaAPP provides commonly used dichotomous and polytomous scoring MIRT models for selection. For dichotomous scoring data, these models include the Rasch model, one-parameter logistic model (1PL), two-parameter logistic model (2PL), three-parameter logistic model (3PL), and four-parameter logistic model (4PL). For polytomous scoring data, the models include the graded response model (GRM), partial credit model (PCM), and generalized partial credit model (GPCM). 

## MIRT Models for Dichotomous Scoring Data



Let $X_{ij}$ represent the response of test taker $j$ ($j = 1,2,3,...,J$) to item $i$ ($i = 1,2,3,...,I$) in dichotomous scoring data, where $X_{ij}$ value is 0 (incorrect) or 1 (correct). Additionally, let $\beta_i$ represent the item difficulty parameter and $\theta_{jq}$ represent the latent trait of test taker $j$ on dimension $q$ ($q = 1,2,3,...,Q$). In IRT analysis, the difficulty parameters and test taker's latent trait parameters have comparability, meaning the difficulty of the item can be determined as easy or difficult for test takers at specific levels, based on the relative sizes of these two parameters.




The specific MIRT models supported by TestAnaAPP for dichotomous scoring data are as follows:



### Multidimensional Rasch Model (M1PL)



The Multidimensional Rasch model is used for dichotomously scored items. The item response function (IRF) of the Multidimensional Rasch model can written as,



$$P(X_{ij} = 1 | \theta_{jq}) = \frac{1}{1+e^{(-(\theta_{jq} - \beta_i))}}\tag{1}$$
Clearly, the Rasch model (1PL) assumes that the probability $P_{ij}$ of subject $j$ answering item $i$ correctly, represented by $X_{ij}=1$, is determined by the relative difficulty of item $i$ to subject $j$, denoted as $\theta_{jq}-\beta_i$.



### Multidimensional Two parameters logistic model (M2PL)



Different from M1PL model, the M2PL model using the parameter, $d_i$, that relate with item parameter $\beta_i$, and item discrimination $\alpha_{iq}$ to describe the psychometric properties of items. The item discrimination $\alpha_{iq}$ represents the $i$th item discrimination for the $q$th dimension. For the between-item test in TestAnaAPP, because each item is considered to be an indicator for a single dimension, the item slopes for all other dimension except for the $q$th dimension are 0. The IRF of the M2PL model is shown as follow,



$$P(X_{ij} = 1 | \theta_{jq}) = \frac{1}{1 + e^{(-(\alpha_{iq}\theta_{jq} + d_i))}}\tag{2}$$

It is important note that the $d_i$ parameter is cannot represent item difficult. To obtain the similar interpretation of item parameters as the unidimensional IRT model, researchers (Reckase, 2009; Zhang & Stone, 2008) proposed the multidimensional discrimination index $MDISC_i$ (Multidimensional Discrimination) to assess the discrimination in MIRT, and the item difficult parameter $\beta_i$ can calculated by the $MDISC_i$ and  $d_i$.

$$MDISC_i = \sqrt{\sum_{q=1}^Q(\alpha_{iq})^2}$$

$$\beta_i = \frac{MDISC_{i}}{-d_i}$$



### Multidimensional Three parameters logistic model (M3PL)



For the M3PL model, the item guessing parameter $c_i$ represents the probability of a subject guessing item $i$ correctly. The probability $P_{ij}$ of subject $j$ answering item $i$ correctly, denoted as $X_{ij}=1$, is determined by the $(\alpha_{iq}\theta_{jq} + d_i)$. The IRF of the M3PL model is shown as follows:



$$P(X_{ij}=1 | \theta_{jq}) = c_i+(1-c_i)\frac{1}{1 + e^{(-(\alpha_{iq}\theta_{jq}
 + d_i))}}\tag{3}$$

Similarly, the item discrimination ($MDISC_i$) and difficult ($\beta_i$) parameters can also calculated by the above mentioned formula.


### Multidimensional Four parameters logistic model (M4PL)



The M3PL model considers the guessing behavior of subjects with extremely low ability on items, while the M4PL model adds consideration for potential errors that may occur among highly capable subjects in addition to the M3PL model. It introduces an additional item error parameter $1-u_i$ to represent the possibility of high-ability subjects making mistakes on item $i$. The formula for the IRF of the M4PL model is shown as follows:



$$P(X_{ij}=1 | \theta_{jq}) = c_i+(u_i-c_i)\frac{1}{1+e^{(-(\alpha_{iq}\theta_{jq}+d_i))}}\tag{4}$$




## MIRT Models for Polytomous Data



Similarly, let $X_{ij}$ represent the response of subject $j (j = 1,2,3,...,J)$ to item $i (i = 1,2,3,...,I)$ in a polytomous data context, where there are $m_i$ response categories. Let $X_{ij}=k$, then $k{\in}{(0,1,2,...,m_i)}$. Additionally, in the polytomous IRT model, the item threshold (difficulty) parameters are denoted as $\beta_{ih}$, where $h{\in}{(1,2,...,m_i)}$, and the latent trait parameter for the subject is denoted as $\theta_{jq}$.



### Multidimensional Partial credit model (MPCM)



The item parameters of the MPCM model only include difficulty parameters $\beta_{ih}$, where $\beta_{ih}$ represents the threshold parameter (difficulty) for category $k$ on item $i$.

The item response function (IRF) for the MPCM model is as follows:



$$P(X_{ikj}=k | \theta_{jq}) = \frac{e^{\sum_{h=0}^{k}(\theta_{jq}-\beta_{ih})}}{\sum_{c=0}^{m_i}e^{\sum_{h=0}^{c}(\theta_{jq}-\beta_{ih})}}\tag{5}$$



where $P(X_{ikj}=k | \theta_{jq})$ represents the probability that subject $j$ responds to item $i$ with category $k$.



### Multidimensional Generalized Partial Credit Model (MGPCM)



The GPCM model allows for the estimation of item discrimination parameters $\alpha_{iq}$. The IRF for the MGPCM model is as follows,



$$P(X_{ikj}=k | \theta_{jq}) = \frac{e^{\sum_{h=0}^{k}[(\alpha_{iq}\theta_{jq}+d_{ih})]}}{\sum_{c=0}^{m_i}e^{\sum_{h=0}^{c}[(\alpha_{iq}\theta_{jq}+d_{ih})]}}\tag{6}$$

where $d_{i}$ is denoting the parameter related item difficult, and can transform to item difficult parameter by using $MDISC_i$ (see above mentioned formula).



### Multidimensional Graded Response Model (MGRM)




Both the MPCM and MGPCM models are based on modeling the probability of response categories $k$ on item $i$. The MGRM model, on the other hand, models the cumulative probability of response categories $k$ or higher. The cumulative response probability function for the MGRM model is as follows:



$$P(X_{ij}{\geq} k | \theta_{jq}) = \frac{e^{[(\alpha_{iq}\theta_{jq}-\beta_{ik})]}}{1+e^{[(\alpha_{iq}\theta_{jq}-\beta_{ik})]}}\tag{7}$$



where $P(X_{ij}{\geq} k | \theta_{jq})$ represents the probability that subject $j$ responds to item $i$ with category $k$ or a higher category. The $\beta_{ik}$ represents the threshold (difficulty) parameter for response category $k$ on item $i$. The probability of a specific response category is obtained by subtracting the adjacent cumulative probabilities:



$$P(X_{ij}= k | \theta_{jq})=P(X_{ij}{\geq} k | \theta_{jq})-P(X_{ij}{\geq} k+1 | \theta_{jq})\tag{8}$$




Please note that unlike the GPCM and PCM models, the GRM model assumes that $\beta_{ik}{\geq}{\beta_{i(k-1)}}$, meaning that the difficulty of higher categories is always greater than that of lower categories.



# Selected Model and Settings



**Based on the settings you selected in the TestAnaAPP interactive program, as well as some default options in TestAnaAPP, the results in this document were estimated according to the following settings:**



* **IRT model: **`r model`；
* **Model parameter estimation method: **`r MIRT_est_method`；
* **Person parameter estimation method: **`r MIRT_person_est_method`；
* **Selected indicator for independence tests: **`r MIRT_select_independent`；
* **Selected indicator for item fit tests: **`r MIRT_itemfit_method`；



# Test Structure

**The test structure (dimension) uploaded from you is shown as follow,**

```{r dimension, echo=FALSE, tab.cap="The test structure in this analysis",tab.id = "realet_fit", tab.cap.style = "Table Caption"}
fmt_tra_other(dimension)
```


# Model Fit



The fitting results of the model included relative fit indices and absolute fit indices. Relative fit indices can be used to compare the fitting degree between models. If two IRT models are used to fit the same set of data and two sets of relative fit index values are obtained, generally, the model that has a lower relative fit index value can be determined as the one that fits the data relatively better. Absolute fit indices represent the absolute fitting degree of the model to the data. If the absolute fit indices are within an acceptable range, it indicates that the model fits the data well.



## Relative Fit Indices



The TestAnaAPP program uses the "mirt" package for parameter estimation and obtains relative fit indices AIC, SABIC, HQ, BIC, and Log-likelihood. Among them, the smaller the values of AIC, SABIC, HQ, and BIC, the higher the fitting degree of the model. The larger the value of Log-likelihood, the higher the fitting degree of the model. If you want to use relative fit indices for IRT model selection, you can choose different models for parameter estimation in TestAnaAPP and record the magnitude of the relative fit index values.



```{r fitindex, echo=FALSE,tab.cap=sprintf("The relative fit indices of the %s", model),tab.id = "realet_fit", tab.cap.style = "Table Caption"}
fmt_tra_other(ft = MIRT_modelfit_relat)

```



## Absolute Fit Indices



The absolute fit indices output by TestAnaAPP include M2, RMSEA, SRMSR, TLI, and CFI. The commonly used criteria in statistics are as follows:



* M2: If the corresponding P value is less than 0.05 ($\alpha<0.05$), the null hypothesis is rejected, indicating that the model does not fit the data.
* RMSEA: Browne and Cudeck (1993) provided a comprehensive guide to RMSEA. A value less than 0.05 indicates a very good fit (close fit), 0.05 to 0.08 indicates a relatively good fit (fair fit), 0.08 to 0.10 indicates a mediocre fit, and a value greater than 0.10 indicates a poor fit. 
* SRMSR: $SRMSR\leq0.05$ indicates that the model adequately fits the data.
* TLI and CFI greater than 0.9 indicate that the model fits the data.




```{r, echo=FALSE,tab.cap=sprintf("The absolute fit indices of the %s", model),tab.id = "ab_fit", tab.cap.style = "Table Caption"}
fmt_tra_other(ft = MIRT_modelfit)

```



# Hypothesis Testing

## Independence Test



TestAnaAPP provides the Q3 and LD-X2 statistics for independent testing in IRT, with larger absolute values indicating stronger dependencies between the two items. Here, due to the lack of uniform standards across different studies, we do not provide the cutoff values for Q3 and LD-X2. We suggest readers refer to professional books or papers for data interpretation.



In this analysis, you selected **`r MIRT_select_independent`** as the statistic for the independence test. 



```{r , echo=FALSE,tab.cap=sprintf("Independence test（%s）",MIRT_select_independent),tab.id = "dependent_test", tab.cap.style = "Table Caption"}
fmt_tra_other(MIRT_Q3,  reset_width = FALSE)
```

<!---BLOCK_LANDSCAPE_STOP--->


# Item Fit



The item fit test is used to determine whether there is a significant difference between the predicted responses of the model and the actual responses. TestAnaAPP provides the $X^2$ statistic for conducting the item fit test. If the $P$ value of the statistical test is less than 0.05, it indicates a significant difference between the predicted values of the model and the actual data. Additionally, if the RMSEA value is greater than 0.1, it suggests a large difference between the predicted values and the actual data. You can make a comprehensive judgment based on the test situation and statistical indicators. It is possible that choosing a different model for the fit may yield different results.



```{r, echo=FALSE,tab.cap= sprintf("The results of item fit (%s-%s)",model, MIRT_itemfit_method),tab.id = "item_fit", tab.cap.style = "Table Caption"}

fmt_tra_other(MIRT_itemfit)
```



# Item Parameters



Item parameters are important indicators used for evaluating the quality of items. Below are the criteria for judging item discrimination values provided by the author in "*THE BASICS OF Item Response Theory in R*". Readers can use these criteria to assess the level of item discrimination in the analyzed test.


`r bold_print("Table: Labels for Item Discrimination Values")``

| Verbal label | Range of values | Typical Value|
|:------------:|:---------------:|:------------:|
| None         | 0              | 0.00 |
| Very low     | 0.01-0.34 | 0.18 |
| Low          | 0.35-0.64 | 0.50 |
| Moderate     | 0.65-1.34 | 1.00 |
| High         | 1.35-1.69 | 1.50 |
| Very high    | >1.70 | 2.00 |
| Perfect      | $+\infty$ | $+\infty$ |



In this analysis, the `r model` model was selected. The `r MIRT_est_method` was used for parameter estimation. Generally, the $MDISC_i$ and $beta_i$ were used to evaluate the discrimination and difficult in MIRT analysis. Thus, the TestAnaAPP converted the $alpha_{iq}$ and $d_i$ parameter as $MDISC_i$ and $beta_i$ respectively. When the test is between-item test, $MDISC_i$ is absolute value of $alpha_{iq}$, because every item has only one $alpha_{i.}$. If the users want to obtain $d_i$ parameter in above formula, whose can use $\beta_i$ calculate $d_i$ according to $d_i = \frac{MDISC_{i}}{-\beta_i}$.

The item parameters are as follows:




```{r, echo=FALSE,tab.cap=sprintf("The item parameters of the %s",model),tab.id = "item_par", tab.cap.style = "Table Caption"}
disc_color <- function(x){
  out <- rep("white", length(x))
  out[x <= 0] <- "red"
  out[x <= 0.3] <- "yellow"
  out
}
diff_color <- function(x){
  out <- rep("white", length(x))
  out[abs(x) > 4 ] <- "yellow"
  out
}
fmt_tra_other(MIRT_itempar)%>%flextable::highlight(j = colnames(MIRT_itempar)%>%
                                                    str_which(pattern = "Discrimination"),
                                                  color = disc_color)%>%
  flextable::highlight(j = colnames(MIRT_itempar)%>%
                         str_which(pattern = "Difficult"),color = diff_color)
```



# Wright Map



In IRT, the difficulty of the items and the latent traits of the participants are measured on the same scale, which allows for comparability. This is a major advantage compared to CTT. The Wright map is a visual tool used to compare the distribution of item difficulty and the distribution of latent traits. By comparing the scatterplot distribution of item difficulty with the distribution of latent trait values, we can determine whether the test is relatively more difficult or easier for the subjects, or whether the difficulty level is moderate. We can also identify which items are more difficult and which are easier. 

It is important noting that only when the test has appropriate difficulty can it effectively differentiate the subjects' differences in latent trait levels. Otherwise, there may be ceiling or floor effects, where the measurement tool cannot accurately assess the subjects' true abilities, leading to all subjects receiving high or low scores.

For multidimensional IRT models, the TestAnaAPP display every dimension's Wright map on the interactive interface. There is a single one, you selected on interactive interface, of `r mode$F_n` Wright maps, you can download the Wright maps on the interactive interface of TestAnaAPP.



```{r ,echo=FALSE,fig.cap=sprintf("The Wright map for the %s model",model), fig.id = "wright",fig.cap.style = "Image Caption",,fig.height=wright_map_height/142,fig.width=plot_width}
print(MIRT_wright) 

```



# Item Characteristic Curve (ICC)



The ICC visualizes the probability of subjects obtaining a certain score based on the model's predictions. By looking at the ICC curve, we can see the probability of subjects at different levels of latent traits obtaining a specific score on each item.

The ICC curve reflects the significance and characteristics of items for subjects at different levels of latent traits. Readers can study how to interpret the ICC curve through specialized books.

For the between-item test that is the TestAnaAPP acceptable test, TestAnaAPP use the single one, corresponding to the item, of several latent traits to draw the ICC figures.



```{r, echo=FALSE,fig.cap=sprintf("The item characteristic curve of this test"), fig.id = "ICC",fig.cap.style = "Image Caption",fig.height=wrap_height_value/142,fig.width=plot_width}
#1cm=28px
#1inch=2.54cm
#1inch=71px
print(MIRT_ICC)
```



# Item Information Curve (IIC)



The item information is an indicator of measurement error at the item level. The higher the value, the greater the contribution of the item to the more accurate measurement of the latent trait of the subjects. The value is often related to discrimination and difficulty. The IIC is a visual representation of the item information, and readers can examine the contribution of the items to the accurate measurement of a specific level of latent trait through the IIC.

Similarly, to learn how to interpret the IIC curve, you may need to seek help from professional books. 

In addition, for the between-item test that is the TestAnaAPP acceptable test, TestAnaAPP use the single one, corresponding to the item, of several latent traits to draw the IIC figures.



```{r, echo=FALSE,fig.cap=sprintf("The item information curve of this test"), fig.id = "IIC",fig.cap.style = "Image Caption",fig.height=wrap_height_value/142,fig.width=plot_width}
print(MIRT_IIC)
```


# Test Information Curve (TIC)



Test information is a measure of test accuracy in IRT, and its value is obtained by summing up the information from each item. A higher value indicates more precise measurement of the latent trait for the subjects. TestAnaAPP provides test information curves and measurement error curves to illustrate test quality. There is a fixed conversion relationship between test information $I(\theta)$ and measurement error $SE$, as shown in the formula:

$$SE = \frac{1}{\sqrt{I(\theta)}}$$



Due to the test is between-item test (each item measures a one of several latent traits) and drawing a picture of $\theta$ in multiple dimensions is not possible, TestAnaAPP draw a single dimension's TIC at a time. Blow is the TIC you selected to draw in the user interface.

```{r, echo=FALSE,fig.cap=sprintf("The test information curve and measurement error"), fig.id = "TIC",fig.cap.style = "Image Caption",fig.height=4,fig.width=plot_width}

print(MIRT_TIC)
```


It is important note that the item information and test information also can download in the user interface, users can draw some custom figures by such downloaded files.

\newpage

# Reference



| Reckase. (2009). Multidimensional Item Response Theory (1st ed. 2009.). Springer New York. https://doi.org/10.1007/978-0-387-89976-3
| Zhang, B & Stone, C. A. (2008). Evaluating Item Fit for Multidimensional Item Response Models. Educational and Psychological Measurement, 68(2), 181–196. https://doi.org/10.1177/0013164407301547
